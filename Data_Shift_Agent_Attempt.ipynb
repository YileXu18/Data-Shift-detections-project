{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "merOzAPNHUQY"
      },
      "source": [
        "# **Installing OpenAI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCb_53ezHLok",
        "outputId": "3ddf0871-aa02-4f31-fd88-59b29264a3fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.2.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOUWjywWHfP6"
      },
      "source": [
        "# **Qualitative Approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1ePiNP-G2-K",
        "outputId": "fd4446ea-3c6c-4465-980d-408207bb88ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here is a detailed checklist to avoid data shift mistakes in machine learning projects:\n",
            "\n",
            "1. **Data Collection:**\n",
            "   - Define the data requirements and objectives of the project.\n",
            "   - Identify relevant data sources and ensure data quality.\n",
            "   - Collect a representative sample of the data to ensure diversity.\n",
            "   - Keep track of metadata, such as timestamps and data sources.\n",
            "\n",
            "2. **Data Preprocessing:**\n",
            "   - Clean the data by handling missing values, outliers, and duplicates.\n",
            "   - Normalize or standardize the data to ensure consistency.\n",
            "   - Perform feature engineering to extract relevant information.\n",
            "   - Split the data into training, validation, and test sets.\n",
            "\n",
            "3. **Model Training:**\n",
            "   - Choose appropriate algorithms based on the problem and data.\n",
            "   - Train the model on the training set using cross-validation techniques.\n",
            "   - Monitor and log hyperparameters, model performance, and training metrics.\n",
            "   - Regularly update and retrain the model as new data becomes available.\n",
            "\n",
            "4. **Validation:**\n",
            "   - Evaluate the model performance on the validation set.\n",
            "   - Monitor for overfitting or underfitting by comparing training and validation metrics.\n",
            "   - Use techniques like cross-validation to ensure robustness of the model.\n",
            "   - Investigate any discrepancies between training and validation performance.\n",
            "\n",
            "5. **Deployment:**\n",
            "   - Prepare the model for deployment by saving the trained model and associated files.\n",
            "   - Implement monitoring and logging mechanisms to track model performance in production.\n",
            "   - Set up a feedback loop to capture real-world data and update the model accordingly.\n",
            "   - Perform A/B testing or gradual rollout to ensure the model performs as expected.\n",
            "\n",
            "6. **Monitoring and Maintenance:**\n",
            "   - Monitor the model's performance in real-time to detect data shift or concept drift.\n",
            "   - Implement automated alerts for potential issues or anomalies.\n",
            "   - Regularly retrain the model on new data to maintain accuracy and relevance.\n",
            "   - Document any changes or updates made to the model for future reference.\n",
            "\n",
            "By following this checklist, you can minimize the risk of data shift mistakes in machine learning projects and ensure the reliability and effectiveness of your models.\n"
          ]
        }
      ],
      "source": [
        "import openai\n",
        "import os\n",
        "\n",
        "openai.api_key = \"*********************************************\"\n",
        "\n",
        "def generate_checklist(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response.choices[0].message[\"content\"].strip()\n",
        "\n",
        "prompt = \"\"\"\n",
        "Create a detailed checklist to avoid data shift mistakes in machine learning projects.\n",
        "The checklist should include steps for data collection, preprocessing, model training, validation, and deployment.\n",
        "\"\"\"\n",
        "\n",
        "checklist = generate_checklist(prompt)\n",
        "print(checklist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtpUOwBcHxue",
        "outputId": "9eee29af-255a-4cdb-b674-9ce04ed90949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Checklist to Avoid Data Shift Mistakes in Machine Learning Projects\n",
            "\n",
            "#### 1. Data Collection:\n",
            "- **Ensure Data Consistency:**\n",
            "  - Define clear data collection protocols and standards.\n",
            "  - Regularly check for duplicates, inconsistencies, and missing values.\n",
            "- **Monitor Changes in Data Sources:**\n",
            "  - Set up automated monitoring systems for data sources.\n",
            "  - Track metadata changes and updates to understand the impact on the data.\n",
            "- **Importance of Version Control:**\n",
            "  - Use version control systems (e.g., Git) to track changes in datasets.\n",
            "  - Document data transformations and maintain a clear record of dataset versions.\n",
            "\n",
            "#### 2. Data Preprocessing:\n",
            "- **Consistent Preprocessing Steps:**\n",
            "  - Standardize data cleaning processes across all datasets.\n",
            "  - Document preprocessing steps to ensure reproducibility.\n",
            "- **Scaling and Normalizing Data:**\n",
            "  - Scale numerical features to ensure uniformity in model training.\n",
            "  - Normalize data to bring different features to a similar scale.\n",
            "- **Check for Missing or Anomalous Values:**\n",
            "  - Implement methods to handle missing values (e.g., imputation, deletion).\n",
            "  - Detect and address outliers or anomalous data points effectively.\n",
            "\n",
            "#### 3. Model Training:\n",
            "- **Use Cross-Validation:**\n",
            "  - Employ cross-validation techniques to evaluate model performance robustly.\n",
            "  - Choose appropriate cross-validation strategies based on data characteristics.\n",
            "- **Training with Representative Data Samples:**\n",
            "  - Ensure training data is representative of real-world scenarios.\n",
            "  - Stratify data sampling to maintain class distribution balance.\n",
            "- **Regular Retraining with Updated Data:**\n",
            "  - Schedule periodic model retraining to incorporate new data.\n",
            "  - Establish automated pipelines for seamless model updates.\n",
            "\n",
            "#### 4. Validation:\n",
            "- **Holdout Validation Set:**\n",
            "  - Create a separate holdout validation set to assess model generalization.\n",
            "  - Validate model performance on unseen data to prevent overfitting.\n",
            "- **Continuous Monitoring of Model Performance Metrics:**\n",
            "  - Monitor key performance indicators (KPIs) regularly to detect performance degradation.\n",
            "  - Implement automated alerts for significant metric deviations.\n",
            "- **Implementation of Statistical Tests for Data Drift Detection:**\n",
            "  - Utilize statistical tests (e.g., Kolmogorov-Smirnov test) to detect data drift.\n",
            "  - Establish thresholds for acceptable drift levels and trigger actions accordingly.\n",
            "\n",
            "#### 5. Deployment:\n",
            "- **Deployment of Monitoring Tools:**\n",
            "  - Deploy monitoring tools\n"
          ]
        }
      ],
      "source": [
        "def generate_checklist(workflow):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": workflow}\n",
        "        ],\n",
        "        max_tokens=500,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "workflow = \"\"\"\n",
        "You are an expert assistant tasked with creating a detailed checklist to avoid data shift mistakes in machine learning projects.\n",
        "The checklist should be comprehensive and divided into the following sections: data collection, preprocessing, model training, validation, and deployment.\n",
        "For each section, provide specific steps, recommendations, and best practices. Here is the structure:\n",
        "\n",
        "1. Data Collection:\n",
        "   - Outline steps to ensure data consistency.\n",
        "   - Detail how to monitor changes in data sources.\n",
        "   - Describe the importance of version control for datasets.\n",
        "\n",
        "2. Data Preprocessing:\n",
        "   - Explain the preprocessing steps that should be consistently applied.\n",
        "   - Discuss the importance of scaling and normalizing data.\n",
        "   - Highlight methods to check for missing or anomalous values.\n",
        "\n",
        "3. Model Training:\n",
        "   - Describe the use of cross-validation to assess model performance.\n",
        "   - Provide guidelines on training models with representative data samples.\n",
        "   - Emphasize the need for regular retraining with updated data.\n",
        "\n",
        "4. Validation:\n",
        "   - Explain how to use a holdout validation set.\n",
        "   - Provide steps to continuously monitor model performance metrics.\n",
        "   - Discuss the implementation of statistical tests to detect data drift.\n",
        "\n",
        "5. Deployment:\n",
        "   - Detail the deployment of monitoring tools to track data and model performance.\n",
        "   - Explain how to set up alerts for significant deviations in data patterns.\n",
        "   - Recommend regular updates to models based on new data insights.\n",
        "\n",
        "Ensure that each section is clear and actionable.\n",
        "\"\"\"\n",
        "\n",
        "# Step 6: Generate the checklist\n",
        "checklist = generate_checklist(workflow)\n",
        "print(checklist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVgEbMl5IRWF"
      },
      "outputs": [],
      "source": [
        "def generate_data_shift_insights(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a data shift detection specialist.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1000,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qupZX-7oITBJ"
      },
      "outputs": [],
      "source": [
        "workflow = \"\"\"\n",
        "You are a data shift detection specialist. Your task is to create a detailed checklist and provide insights to detect data shift in machine learning projects. The checklist should cover the following areas: data collection, data preprocessing, model training, model validation, and model deployment. For each area, provide specific steps, recommendations, best practices, and methods to detect data shift. Include examples and tools where applicable.\n",
        "\n",
        "1. Data Collection:\n",
        "   - Steps to ensure data consistency.\n",
        "   - Methods to monitor changes in data sources.\n",
        "   - Importance and implementation of version control for datasets.\n",
        "\n",
        "2. Data Preprocessing:\n",
        "   - Preprocessing steps that should be consistently applied.\n",
        "   - Techniques for scaling and normalizing data.\n",
        "   - Methods to check for missing or anomalous values.\n",
        "   - Tools and techniques for detecting changes in data distribution.\n",
        "\n",
        "3. Model Training:\n",
        "   - Use of cross-validation to assess model performance.\n",
        "   - Guidelines on training models with representative data samples.\n",
        "   - Regular retraining strategies with updated data.\n",
        "   - Techniques for detecting data shift during training.\n",
        "\n",
        "4. Model Validation:\n",
        "   - Use of a holdout validation set and its importance.\n",
        "   - Continuous monitoring of model performance metrics.\n",
        "   - Statistical tests to detect data drift.\n",
        "   - Tools and techniques for validation and monitoring.\n",
        "\n",
        "5. Model Deployment:\n",
        "   - Deployment of monitoring tools to track data and model performance.\n",
        "   - Setting up alerts for significant deviations in data patterns.\n",
        "   - Regular updates to models based on new data insights.\n",
        "   - Strategies for handling detected data shifts in production.\n",
        "\n",
        "Provide detailed steps and explanations for each point, along with examples and recommended tools.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-FIdjZJIU4m",
        "outputId": "ab225553-c4ab-4836-80fe-a11b670a62b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Data Collection:\n",
            "   - Steps to ensure data consistency:\n",
            "     - Define clear data collection processes and protocols.\n",
            "     - Regularly check for missing or incomplete data.\n",
            "     - Implement data quality checks to identify inconsistencies.\n",
            "   - Methods to monitor changes in data sources:\n",
            "     - Set up automated monitoring systems to track data sources.\n",
            "     - Compare new data with historical data to detect deviations.\n",
            "     - Use data profiling tools to analyze data distributions.\n",
            "   - Importance and implementation of version control for datasets:\n",
            "     - Use version control systems (e.g., Git) to track changes in datasets.\n",
            "     - Document dataset versions and changes made during preprocessing.\n",
            "     - Ensure reproducibility by maintaining a record of dataset versions.\n",
            "\n",
            "2. Data Preprocessing:\n",
            "   - Preprocessing steps that should be consistently applied:\n",
            "     - Cleaning data by handling missing values and outliers.\n",
            "     - Encoding categorical variables and standardizing data formats.\n",
            "     - Feature scaling and normalization for model performance.\n",
            "   - Techniques for scaling and normalizing data:\n",
            "     - Standard scaling, Min-Max scaling, or Robust scaling methods.\n",
            "     - Use of sklearn.preprocessing module in Python for preprocessing.\n",
            "   - Methods to check for missing or anomalous values:\n",
            "     - Utilize descriptive statistics to identify missing values.\n",
            "     - Implement outlier detection techniques (e.g., Z-score, IQR).\n",
            "     - Visualize data distributions to spot anomalies.\n",
            "   - Tools and techniques for detecting changes in data distribution:\n",
            "     - Statistical tests like Kolmogorov-Smirnov or Chi-Square tests.\n",
            "     - Data drift detection algorithms (e.g., Drift Detection Method).\n",
            "     - Data monitoring platforms like Datadog or Grafana.\n",
            "\n",
            "3. Model Training:\n",
            "   - Use of cross-validation to assess model performance:\n",
            "     - Perform k-fold cross-validation to evaluate model generalization.\n",
            "     - Ensure each fold represents the overall data distribution.\n",
            "   - Guidelines on training models with representative data samples:\n",
            "     - Stratified sampling to maintain class distribution in training data.\n",
            "     - Consider data augmentation techniques to enhance model robustness.\n",
            "   - Regular retraining strategies with updated data:\n",
            "     - Set up automated pipelines for periodic model retraining.\n",
            "     - Monitor model performance metrics to trigger retraining.\n",
            "   - Techniques for detecting data shift during training:\n",
            "     - Compare model performance metrics between training iterations.\n",
            "     - Use statistical tests to detect significant changes in model behavior.\n",
            "\n",
            "4. Model Validation:\n",
            "   - Use of a holdout validation set and its importance:\n",
            "     - Reserve a portion of data for validation to assess model generalization.\n",
            "     - Avoid overfitting by evaluating on unseen data.\n",
            "   - Continuous monitoring of model performance metrics:\n",
            "     - Track key performance indicators (KPIs) such as accuracy, precision, recall.\n",
            "     - Set up monitoring dashboards to visualize model performance.\n",
            "   - Statistical tests to detect data drift:\n",
            "     - Use hypothesis testing (e.g., KS test, t-test) to detect drift in predictions.\n",
            "     - Monitor feature distributions for shifts over time.\n",
            "   - Tools and techniques for validation and monitoring:\n",
            "     - MLflow for experiment tracking and model validation.\n",
            "     - TensorBoard for visualizing model training and validation metrics.\n",
            "\n",
            "5. Model Deployment:\n",
            "   - Deployment of monitoring tools to track data and model performance:\n",
            "     - Implement logging and monitoring systems (e.g., ELK stack).\n",
            "     - Use anomaly detection algorithms to flag unusual model behavior.\n",
            "   - Setting up alerts for significant deviations in data patterns:\n",
            "     - Define thresholds for data drift detection and trigger alerts.\n",
            "     - Integrate monitoring tools with alerting systems (e.g., Prometheus).\n",
            "   - Regular updates to models based on new data insights:\n",
            "     - Schedule periodic model re-evaluations based on performance.\n",
            "     - Implement feedback loops to incorporate new data into model updates.\n",
            "   - Strategies for handling detected data shifts in production:\n",
            "     - Have rollback mechanisms in place to revert to previous models.\n",
            "     - Implement automated retraining pipelines to adapt to data shifts.\n",
            "\n",
            "By following this detailed checklist and incorporating the recommended steps, practices, and tools, data shift detection in machine learning projects can be effectively managed to ensure model performance and reliability.\n"
          ]
        }
      ],
      "source": [
        "insights = generate_data_shift_insights(workflow)\n",
        "print(insights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lh4NzOpbHxzq"
      },
      "source": [
        "# **Quantitative Approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XvUlQYMiI-Sz",
        "outputId": "9eb1ef02-b0cf-4fdd-a0ee-7c872c871e9c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3a93d417-b102-4ab6-81d9-56d20e6809df\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3a93d417-b102-4ab6-81d9-56d20e6809df\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving SFPD_stop_cleaned_data.csv to SFPD_stop_cleaned_data.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-b9327ae6c824>:12: DtypeWarning: Columns (27,28,41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(file_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data Shift Analysis Report:\n",
            "\n",
            "Column 'doj_record_id' shows a potential data shift.\n",
            "  - Statistic: 1.6714001356176482\n",
            "\n",
            "Column 'person_number' does not show significant data shift.\n",
            "  - Statistic: 0.0024236000901841637\n",
            "  - P-value: 0.8553208407192856\n",
            "\n",
            "Column 'agency_ori' does not show significant data shift.\n",
            "  - Statistic: 0.0\n",
            "\n",
            "Column 'stop_datetime' shows a potential data shift.\n",
            "  - Statistic: 1.5040839368408818\n",
            "\n",
            "Column 'duration_of_stop' does not show significant data shift.\n",
            "  - Statistic: 0.0025625565273678097\n",
            "  - P-value: 0.8051680767666806\n",
            "\n",
            "Column 'is_stop_response_to_call' does not show significant data shift.\n",
            "  - Statistic: 0.0008416639895324485\n",
            "  - P-value: 0.9999999999886952\n",
            "\n",
            "Column 'location' shows a potential data shift.\n",
            "  - Statistic: 0.561321733344526\n",
            "\n",
            "Column 'district' does not show significant data shift.\n",
            "  - Statistic: 9.190702430430465e-05\n",
            "\n",
            "Column 'city' does not show significant data shift.\n",
            "  - Statistic: 0.0013663455115759111\n",
            "\n",
            "Column 'perceived_race_ethnicity' does not show significant data shift.\n",
            "  - Statistic: 3.163915621650343e-05\n",
            "\n",
            "Column 'perceived_gender' does not show significant data shift.\n",
            "  - Statistic: 6.27593515317447e-05\n",
            "\n",
            "Column 'is_lgbt' does not show significant data shift.\n",
            "  - Statistic: 0.00011982935837684394\n",
            "  - P-value: 1.0\n",
            "\n",
            "Column 'perceived_age' does not show significant data shift.\n",
            "  - Statistic: 0.0024368324195003233\n",
            "  - P-value: 0.8507915458172264\n",
            "\n",
            "Column 'perceived_age_group' does not show significant data shift.\n",
            "  - Statistic: 4.6688265183632924e-05\n",
            "\n",
            "Column 'had_limited_or_no_english' does not show significant data shift.\n",
            "  - Statistic: 0.000119555258345172\n",
            "  - P-value: 1.0\n",
            "\n",
            "Column 'perceived_or_known_disability' does not show significant data shift.\n",
            "  - Statistic: 0.0009489945421469131\n",
            "\n",
            "Column 'reason_for_stop' does not show significant data shift.\n",
            "  - Statistic: 5.391720337207396e-05\n",
            "\n",
            "Column 'traffic_violation_type' does not show significant data shift.\n",
            "  - Statistic: 4.40635891717169e-05\n",
            "\n",
            "Column 'traffic_viol_cjis_off_code' does not show significant data shift.\n",
            "  - Statistic: 0.00445754684036892\n",
            "  - P-value: 0.16612242001211963\n",
            "\n",
            "Column 'traffic_viol_off_code' does not show significant data shift.\n",
            "  - Statistic: 0.007481624209050224\n",
            "\n",
            "Column 'traffic_viol_off_statute' does not show significant data shift.\n",
            "  - Statistic: 0.007381347355929652\n",
            "\n",
            "Column 'suspicion_cjis_off_code' does not show significant data shift.\n",
            "  - Statistic: 0.0027692679854407487\n",
            "  - P-value: 0.7226733150837265\n",
            "\n",
            "Column 'suspicion_off_code_txt' does not show significant data shift.\n",
            "  - Statistic: 0.031031502239048006\n",
            "\n",
            "Column 'suspicion_off_statute' does not show significant data shift.\n",
            "  - Statistic: 0.02854244923443046\n",
            "\n",
            "Column 'suspicion_sub_type' does not show significant data shift.\n",
            "  - Statistic: 0.005128294392566897\n",
            "\n",
            "Column 'actions_taken' does not show significant data shift.\n",
            "  - Statistic: 0.04467640902430306\n",
            "\n",
            "Column 'basis_for_search' does not show significant data shift.\n",
            "  - Statistic: 0.017345622280236724\n",
            "\n",
            "Column 'basis_for_property_seizure' does not show significant data shift.\n",
            "  - Statistic: 0.004921631602260187\n",
            "\n",
            "Column 'type_of_property_seized' shows a potential data shift.\n",
            "  - Statistic: 0.06339004422740116\n",
            "\n",
            "Column 'contraband_or_evidence' does not show significant data shift.\n",
            "  - Statistic: 0.02784029628298417\n",
            "\n",
            "Column 'results_of_stop' does not show significant data shift.\n",
            "  - Statistic: 0.0008134844678907177\n",
            "\n",
            "Column 'longitude' does not show significant data shift.\n",
            "  - Statistic: 0.002148073088397706\n",
            "  - P-value: 0.9345541376928642\n",
            "\n",
            "Column 'latitude' does not show significant data shift.\n",
            "  - Statistic: 0.0025890693421650735\n",
            "  - P-value: 0.7950242465442932\n",
            "\n",
            "Column 'supervisor_district' does not show significant data shift.\n",
            "  - Statistic: 0.0023400862999997663\n",
            "  - P-value: 0.8825154365909721\n",
            "\n",
            "Column 'analysis_neighborhoods' does not show significant data shift.\n",
            "  - Statistic: 0.00036622401109054467\n",
            "\n",
            "Column 'perceived_race_ethnicity_code' does not show significant data shift.\n",
            "  - Statistic: 0.0009867123270821832\n",
            "\n",
            "Column 'perceived_gender_code' does not show significant data shift.\n",
            "  - Statistic: 0.0013612553256835414\n",
            "  - P-value: 0.9998181771468815\n",
            "\n",
            "Column 'perceived_or_known_disability_code' does not show significant data shift.\n",
            "  - Statistic: 0.0003251594163164157\n",
            "\n",
            "Column 'reason_for_stop_code' does not show significant data shift.\n",
            "  - Statistic: 0.004001615588306806\n",
            "  - P-value: 0.26865762410067495\n",
            "\n",
            "Column 'actions_taken_code' does not show significant data shift.\n",
            "  - Statistic: 0.018237184485707852\n",
            "\n",
            "Column 'basis_for_search_code' does not show significant data shift.\n",
            "  - Statistic: 0.01734562228023672\n",
            "\n",
            "Column 'basis_for_property_seizure_code' does not show significant data shift.\n",
            "  - Statistic: 0.005077867462302178\n",
            "\n",
            "Column 'type_of_property_seized_code' shows a potential data shift.\n",
            "  - Statistic: 0.06339004422740116\n",
            "\n",
            "Column 'contraband_or_evidence_code' does not show significant data shift.\n",
            "  - Statistic: 0.0028198649172720314\n",
            "\n",
            "Column 'suspicion_sub_type_code' does not show significant data shift.\n",
            "  - Statistic: 0.005128294392566897\n",
            "\n",
            "Column 'results_of_stop_code' does not show significant data shift.\n",
            "  - Statistic: 0.0008134844678907177\n",
            "\n",
            "\n",
            "LLM Insights and Recommendations:\n",
            "\n",
            "Based on the data shift analysis report provided, it is evident that there are several columns showing potential data shifts. These columns include 'doj_record_id', 'stop_datetime', 'location', 'type_of_property_seized', and 'type_of_property_seized_code'. These potential data shifts may indicate changes or anomalies in the data distribution within these columns.\n",
            "\n",
            "Additional Insights:\n",
            "1. The 'doj_record_id' column showing a potential data shift could imply changes in the unique identifiers or records within the dataset. Further investigation is recommended to understand the nature of this shift.\n",
            "\n",
            "2. The 'stop_datetime' column showing a potential data shift may suggest variations in the timing or frequency of stops recorded. It would be beneficial to explore the reasons behind this shift.\n",
            "\n",
            "3. The 'location' column displaying a potential data shift could indicate differences in the geographical distribution of stops. Understanding the factors contributing to this shift is essential for accurate analysis.\n",
            "\n",
            "4. The 'type_of_property_seized' and 'type_of_property_seized_code' columns showing potential data shifts may signify changes in the types of properties seized during stops. Investigating the reasons behind these shifts can provide valuable insights into law enforcement activities.\n",
            "\n",
            "Recommendations:\n",
            "1. Conduct a detailed investigation into the columns showing potential data shifts to identify the root causes and determine if these shifts are expected or unexpected.\n",
            "\n",
            "2. Implement monitoring mechanisms to track changes in the data distribution over time, especially in columns with potential data shifts, to ensure data integrity and consistency.\n",
            "\n",
            "3. Collaborate with domain experts to gain a deeper understanding of the context surrounding the potential data shifts and to interpret their implications accurately.\n",
            "\n",
            "4. Consider performing further statistical analyses or data visualization techniques to explore the relationships between the columns showing potential data shifts and other relevant variables in the dataset.\n",
            "\n",
            "By following these recommendations and gaining additional insights into the potential data shifts identified in the report, you can enhance the quality and reliability of your data analysis processes.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "import os\n",
        "\n",
        "def read_csv(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "def analyze_data_shift(df):\n",
        "\n",
        "    train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "\n",
        "    shift_report = {}\n",
        "\n",
        "    for column in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[column]):\n",
        "\n",
        "            stat, p_value = ks_2samp(train_df[column], test_df[column])\n",
        "            shift_report[column] = {\n",
        "                \"statistic\": stat,\n",
        "                \"p_value\": p_value,\n",
        "                \"shift_detected\": p_value < 0.05\n",
        "            }\n",
        "        else:\n",
        "\n",
        "            train_dist = train_df[column].value_counts(normalize=True)\n",
        "            test_dist = test_df[column].value_counts(normalize=True)\n",
        "            all_categories = set(train_dist.index).union(set(test_dist.index))\n",
        "            train_dist = train_dist.reindex(all_categories, fill_value=0)\n",
        "            test_dist = test_dist.reindex(all_categories, fill_value=0)\n",
        "            stat = np.sum((train_dist - test_dist)**2 / (train_dist + test_dist + 1e-6))\n",
        "            shift_report[column] = {\n",
        "                \"statistic\": stat,\n",
        "                \"shift_detected\": stat > 0.05\n",
        "            }\n",
        "\n",
        "    return shift_report\n",
        "\n",
        "def generate_shift_insights(shift_report):\n",
        "    insights = \"Data Shift Analysis Report:\\n\\n\"\n",
        "    for column, report in shift_report.items():\n",
        "        if report[\"shift_detected\"]:\n",
        "            insights += f\"Column '{column}' shows a potential data shift.\\n\"\n",
        "            insights += f\"  - Statistic: {report['statistic']}\\n\"\n",
        "            if \"p_value\" in report:\n",
        "                insights += f\"  - P-value: {report['p_value']}\\n\"\n",
        "            insights += \"\\n\"\n",
        "        else:\n",
        "            insights += f\"Column '{column}' does not show significant data shift.\\n\"\n",
        "            insights += f\"  - Statistic: {report['statistic']}\\n\"\n",
        "            if \"p_value\" in report:\n",
        "                insights += f\"  - P-value: {report['p_value']}\\n\"\n",
        "            insights += \"\\n\"\n",
        "\n",
        "    return insights\n",
        "\n",
        "def get_llm_insights(prompt):\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a data shift detection specialist.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        max_tokens=1000,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    return response['choices'][0]['message']['content']\n",
        "\n",
        "\n",
        "def analyze_csv_for_data_shift(file_path):\n",
        "\n",
        "    df = read_csv(file_path)\n",
        "\n",
        "\n",
        "    shift_report = analyze_data_shift(df)\n",
        "\n",
        "\n",
        "    shift_insights = generate_shift_insights(shift_report)\n",
        "\n",
        "\n",
        "    llm_prompt = f\"Here is the data shift analysis report:\\n\\n{shift_insights}\\nPlease provide additional insights and recommendations based on this report.\"\n",
        "    llm_insights = get_llm_insights(llm_prompt)\n",
        "\n",
        "\n",
        "    final_insights = f\"{shift_insights}\\nLLM Insights and Recommendations:\\n\\n{llm_insights}\"\n",
        "    return final_insights\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]\n",
        "\n",
        "\n",
        "insights = analyze_csv_for_data_shift(file_path)\n",
        "print(insights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiUzPQkpG-Rp"
      },
      "source": [
        "# **Improving Functions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjXKkL0aO-2B",
        "outputId": "440b62ff-193e-4f1b-c594-b7c3cb2f0fb3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-b58e4fb0b2bb>:8: DtypeWarning: Columns (27,28,41,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  return pd.read_csv(file_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                Column  Statistic P-value/Shift Detected  \\\n",
            "0                        doj_record_id   1.792558                   True   \n",
            "1                        person_number   0.002184               0.945202   \n",
            "2                           agency_ori   0.000000                  False   \n",
            "3                        stop_datetime   1.812836                   True   \n",
            "4                     duration_of_stop   0.109771                    0.0   \n",
            "5             is_stop_response_to_call   0.082368                    0.0   \n",
            "6                             location   1.323937                   True   \n",
            "7                             district   0.025604                  False   \n",
            "8                                 city   0.017861                  False   \n",
            "9             perceived_race_ethnicity   0.003299                  False   \n",
            "10                    perceived_gender   0.000987                  False   \n",
            "11                             is_lgbt   0.000118                    1.0   \n",
            "12                       perceived_age   0.018142                    0.0   \n",
            "13                 perceived_age_group   0.002390                  False   \n",
            "14           had_limited_or_no_english   0.021267                    0.0   \n",
            "15       perceived_or_known_disability   0.010645                  False   \n",
            "16                     reason_for_stop   0.007250                  False   \n",
            "17              traffic_violation_type   0.016294                  False   \n",
            "18          traffic_viol_cjis_off_code   0.083344                    0.0   \n",
            "19               traffic_viol_off_code   0.101014                   True   \n",
            "20            traffic_viol_off_statute   0.112899                   True   \n",
            "21             suspicion_cjis_off_code   0.043090                    0.0   \n",
            "22              suspicion_off_code_txt   0.267327                   True   \n",
            "23               suspicion_off_statute   0.246272                   True   \n",
            "24                  suspicion_sub_type   0.073143                   True   \n",
            "25                       actions_taken   0.156881                   True   \n",
            "26                    basis_for_search   0.098605                   True   \n",
            "27          basis_for_property_seizure   0.012621                  False   \n",
            "28             type_of_property_seized   0.150048                   True   \n",
            "29              contraband_or_evidence   0.151107                   True   \n",
            "30                     results_of_stop   0.032260                  False   \n",
            "31                           longitude   0.015097                    0.0   \n",
            "32                            latitude   0.037206                    0.0   \n",
            "33                 supervisor_district   0.030109                    0.0   \n",
            "34              analysis_neighborhoods   0.016775                  False   \n",
            "35       perceived_race_ethnicity_code   0.004859                  False   \n",
            "36               perceived_gender_code   0.005757               0.043364   \n",
            "37  perceived_or_known_disability_code   0.001789                  False   \n",
            "38                reason_for_stop_code   0.047343                    0.0   \n",
            "39                  actions_taken_code   0.073189                   True   \n",
            "40               basis_for_search_code   0.098605                   True   \n",
            "41     basis_for_property_seizure_code   0.012767                  False   \n",
            "42        type_of_property_seized_code   0.150048                   True   \n",
            "43         contraband_or_evidence_code   0.024404                  False   \n",
            "44             suspicion_sub_type_code   0.073143                   True   \n",
            "45                results_of_stop_code   0.032260                  False   \n",
            "46                                year   1.000000                    0.0   \n",
            "\n",
            "   Shift Detected  \n",
            "0            None  \n",
            "1           False  \n",
            "2            None  \n",
            "3            None  \n",
            "4            True  \n",
            "5            True  \n",
            "6            None  \n",
            "7            None  \n",
            "8            None  \n",
            "9            None  \n",
            "10           None  \n",
            "11          False  \n",
            "12           True  \n",
            "13           None  \n",
            "14           True  \n",
            "15           None  \n",
            "16           None  \n",
            "17           None  \n",
            "18           True  \n",
            "19           None  \n",
            "20           None  \n",
            "21           True  \n",
            "22           None  \n",
            "23           None  \n",
            "24           None  \n",
            "25           None  \n",
            "26           None  \n",
            "27           None  \n",
            "28           None  \n",
            "29           None  \n",
            "30           None  \n",
            "31           True  \n",
            "32           True  \n",
            "33           True  \n",
            "34           None  \n",
            "35           None  \n",
            "36           True  \n",
            "37           None  \n",
            "38           True  \n",
            "39           None  \n",
            "40           None  \n",
            "41           None  \n",
            "42           None  \n",
            "43           None  \n",
            "44           None  \n",
            "45           None  \n",
            "46           True  \n"
          ]
        }
      ],
      "source": [
        "def extract_year_from_datetime(df):\n",
        "    df['year'] = pd.to_datetime(df['stop_datetime']).dt.year\n",
        "    return df\n",
        "\n",
        "def analyze_data_shift(df):\n",
        "\n",
        "    df = extract_year_from_datetime(df)\n",
        "\n",
        "\n",
        "    train_df = df[df['year'] < 2020]\n",
        "    test_df = df[df['year'] >= 2020]\n",
        "\n",
        "    shift_report = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        if pd.api.types.is_numeric_dtype(df[column]):\n",
        "\n",
        "            stat, p_value = ks_2samp(train_df[column], test_df[column])\n",
        "            shift_report.append([column, stat, p_value, p_value < 0.05])\n",
        "        else:\n",
        "\n",
        "            train_dist = train_df[column].value_counts(normalize=True)\n",
        "            test_dist = test_df[column].value_counts(normalize=True)\n",
        "            all_categories = set(train_dist.index).union(set(test_dist.index))\n",
        "            train_dist = train_dist.reindex(all_categories, fill_value=0)\n",
        "            test_dist = test_dist.reindex(all_categories, fill_value=0)\n",
        "            stat = np.sum((train_dist - test_dist)**2 / (train_dist + test_dist + 1e-6))\n",
        "            shift_report.append([column, stat, stat > 0.05])\n",
        "\n",
        "\n",
        "    shift_report_df = pd.DataFrame(shift_report, columns=['Column', 'Statistic', 'P-value/Shift Detected', 'Shift Detected'])\n",
        "\n",
        "    return shift_report_df\n",
        "\n",
        "\n",
        "file_path = \"/content/SFPD_stop_cleaned_data.csv\"\n",
        "df = read_csv(file_path)\n",
        "shift_report = analyze_data_shift(df)\n",
        "print(shift_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tt_QhapPfq0",
        "outputId": "7bdf724a-0200-40e5-96c8-6b39c43c328b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'doj_record_id': {'statistic': 1.7925581299440647, 'shift_detected': True},\n",
              " 'person_number': {'statistic': 0.0021844359575853654,\n",
              "  'p_value': 0.9452016604771285,\n",
              "  'shift_detected': False},\n",
              " 'agency_ori': {'statistic': 0.0, 'shift_detected': False},\n",
              " 'stop_datetime': {'statistic': 1.8128361351349973, 'shift_detected': True},\n",
              " 'duration_of_stop': {'statistic': 0.10977126857837027,\n",
              "  'p_value': 0.0,\n",
              "  'shift_detected': True},\n",
              " 'is_stop_response_to_call': {'statistic': 0.0823678529583658,\n",
              "  'p_value': 0.0,\n",
              "  'shift_detected': True},\n",
              " 'location': {'statistic': 1.3239370649277609, 'shift_detected': True},\n",
              " 'district': {'statistic': 0.02560410772798309, 'shift_detected': False},\n",
              " 'city': {'statistic': 0.01786080768295122, 'shift_detected': False},\n",
              " 'perceived_race_ethnicity': {'statistic': 0.0032986942868011688,\n",
              "  'shift_detected': False},\n",
              " 'perceived_gender': {'statistic': 0.0009873894904830903,\n",
              "  'shift_detected': False},\n",
              " 'is_lgbt': {'statistic': 0.00011782226263068463,\n",
              "  'p_value': 1.0,\n",
              "  'shift_detected': False},\n",
              " 'perceived_age': {'statistic': 0.018141557905498207,\n",
              "  'p_value': 6.167698563324245e-17,\n",
              "  'shift_detected': True},\n",
              " 'perceived_age_group': {'statistic': 0.0023900573034427923,\n",
              "  'shift_detected': False},\n",
              " 'had_limited_or_no_english': {'statistic': 0.021267468021469282,\n",
              "  'p_value': 4.0783531261898477e-23,\n",
              "  'shift_detected': True},\n",
              " 'perceived_or_known_disability': {'statistic': 0.01064496470533069,\n",
              "  'shift_detected': False},\n",
              " 'reason_for_stop': {'statistic': 0.00724995377253689,\n",
              "  'shift_detected': False},\n",
              " 'traffic_violation_type': {'statistic': 0.016294464622426296,\n",
              "  'shift_detected': False},\n",
              " 'traffic_viol_cjis_off_code': {'statistic': 0.08334376188626891,\n",
              "  'p_value': 0.0,\n",
              "  'shift_detected': True},\n",
              " 'traffic_viol_off_code': {'statistic': 0.10101384523641255,\n",
              "  'shift_detected': True},\n",
              " 'traffic_viol_off_statute': {'statistic': 0.11289936707441374,\n",
              "  'shift_detected': True},\n",
              " 'suspicion_cjis_off_code': {'statistic': 0.043090272805231145,\n",
              "  'p_value': 1.3711803633444318e-93,\n",
              "  'shift_detected': True},\n",
              " 'suspicion_off_code_txt': {'statistic': 0.2673273457837452,\n",
              "  'shift_detected': True},\n",
              " 'suspicion_off_statute': {'statistic': 0.2462715515545905,\n",
              "  'shift_detected': True},\n",
              " 'suspicion_sub_type': {'statistic': 0.07314335633419025,\n",
              "  'shift_detected': True},\n",
              " 'actions_taken': {'statistic': 0.15688128831255288, 'shift_detected': True},\n",
              " 'basis_for_search': {'statistic': 0.09860548467747375,\n",
              "  'shift_detected': True},\n",
              " 'basis_for_property_seizure': {'statistic': 0.012621100271737986,\n",
              "  'shift_detected': False},\n",
              " 'type_of_property_seized': {'statistic': 0.15004835066314523,\n",
              "  'shift_detected': True},\n",
              " 'contraband_or_evidence': {'statistic': 0.1511073856273429,\n",
              "  'shift_detected': True},\n",
              " 'results_of_stop': {'statistic': 0.032260416426087755,\n",
              "  'shift_detected': False},\n",
              " 'longitude': {'statistic': 0.015096942501002908,\n",
              "  'p_value': 7.353802017739933e-12,\n",
              "  'shift_detected': True},\n",
              " 'latitude': {'statistic': 0.03720570541651269,\n",
              "  'p_value': 7.097021861417972e-70,\n",
              "  'shift_detected': True},\n",
              " 'supervisor_district': {'statistic': 0.030109492048444486,\n",
              "  'p_value': 6.608249110625444e-46,\n",
              "  'shift_detected': True},\n",
              " 'analysis_neighborhoods': {'statistic': 0.016775298077078456,\n",
              "  'shift_detected': False},\n",
              " 'perceived_race_ethnicity_code': {'statistic': 0.00485885144492725,\n",
              "  'shift_detected': False},\n",
              " 'perceived_gender_code': {'statistic': 0.005757292911398815,\n",
              "  'p_value': 0.043363725392867924,\n",
              "  'shift_detected': True},\n",
              " 'perceived_or_known_disability_code': {'statistic': 0.0017890827760876256,\n",
              "  'shift_detected': False},\n",
              " 'reason_for_stop_code': {'statistic': 0.04734316598660859,\n",
              "  'p_value': 6.78013985997044e-113,\n",
              "  'shift_detected': True},\n",
              " 'actions_taken_code': {'statistic': 0.07318851701030762,\n",
              "  'shift_detected': True},\n",
              " 'basis_for_search_code': {'statistic': 0.09860548467747374,\n",
              "  'shift_detected': True},\n",
              " 'basis_for_property_seizure_code': {'statistic': 0.012767494926764817,\n",
              "  'shift_detected': False},\n",
              " 'type_of_property_seized_code': {'statistic': 0.15004835066314523,\n",
              "  'shift_detected': True},\n",
              " 'contraband_or_evidence_code': {'statistic': 0.024403814900442283,\n",
              "  'shift_detected': False},\n",
              " 'suspicion_sub_type_code': {'statistic': 0.07314335633419025,\n",
              "  'shift_detected': True},\n",
              " 'results_of_stop_code': {'statistic': 0.032260416426087755,\n",
              "  'shift_detected': False},\n",
              " 'year': {'statistic': 1.0, 'p_value': 0.0, 'shift_detected': True}}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "shift_report"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
